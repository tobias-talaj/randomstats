{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('G:\\python_github_dump\\github_python_dump000000000001', encoding='utf8') as f:\n",
    "    dump = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = dump.replace('\",false,1', '')\n",
    "splitted = re.split(r'[a-z0-9]{40},\\d+,\"', dump)\n",
    "splitted = splitted[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\"\"\"\n",
      "Base class for Gaussian process latent variable models\n",
      "This is really not ready for release yet but is used by the gpasso model\n",
      "\"\"\"\"\"\"\n",
      "import sys\n",
      "sys.path.append('./..')\n",
      "\n",
      "from pygp.gpr import GP,_solve_chol,optHyper\n",
      "import scipy as SP\n",
      "import numpy.linalg as linalg\n",
      "import pdb\n",
      "import copy\n",
      "\n",
      "\n",
      "def grad_checkK(K,logtheta,x1,dimensions):\n",
      "    \"\"\"\"\"\"perform grad check\"\"\"\"\"\"\n",
      "    from pyplot import *\n",
      "    L=0;\n",
      "    relchange = 1E-5;\n",
      "    x1_0 = x1.copy()\n",
      "    n = x1.shape[0]\n",
      "    nd = len(dimensions)\n",
      "    diff = SP.zeros([n,nd,n,n])\n",
      "    for i in xrange(n):\n",
      "        for iid in xrange(nd):\n",
      "            d = dimensions[iid] \n",
      "            change = relchange*x1_0[i,d]\n",
      "            change = max(change,1E-5)\n",
      "            x1[i,d] = x1_0[i,d] + change\n",
      "            Lplus = K.K(logtheta,x1,x1)\n",
      "            x1[i,d] = x1_0[i,d] - change\n",
      "            Lminus = K.K(logtheta,x1,x1)\n",
      "            x1[i,d] = x1_0[i,d]\n",
      "\n",
      "            diff[i,iid,:,:] = (Lplus-Lminus)/(2*change)\n",
      "    #anal\n",
      "    anal = SP.zeros([n,nd,n,n])\n",
      "    for iid in xrange(nd):\n",
      "        d = dimensions[iid]\n",
      "        dKx = K.Kd_dx(logtheta,x1,x1,d)\n",
      "        dKx_diag = K.Kd_dx_diag(logtheta,x1,d)\n",
      "        for i in xrange(n):\n",
      "            anal[i,iid,i,:] = dKx[i,:]\n",
      "            anal[i,iid,:,i] = dKx[i,:]\n",
      "            anal[i,iid,i,i] = dKx_diag[i]\n",
      "            \n",
      "    delta = anal -diff\n",
      "    print \"\"delta\"\"\n",
      "    pdb.set_trace()\n",
      "    pass\n",
      "\n",
      "\n",
      "def PCA(Y,components):\n",
      "    \"\"\"\"\"\"run PCA, retrieving the first (components) principle components\n",
      "    return [s0,w0]\n",
      "    s0: factors\n",
      "    w0: weights\n",
      "    \"\"\"\"\"\"\n",
      "    sv = linalg.svd(Y, full_matrices = 0);\n",
      "    [s0,w0] = [sv[0][:,0:components], SP.dot(SP.diag(sv[1]),sv[2]).T[:,0:components]]\n",
      "    v = s0.std(axis=0)\n",
      "    s0 /= v;\n",
      "    w0 *= v;\n",
      "    return [s0,w0]\n",
      "\n",
      "    \n",
      "class GPLVM(GP):\n",
      "    \"\"\"\"\"\"\n",
      "    derived class form GP offering GPLVM specific functionality\n",
      "    \n",
      "    \n",
      "    \"\"\"\"\"\"\n",
      "    __slots__ = [\"\"gplvm_dimensions\"\"]\n",
      "    \n",
      "    def __init__(self,**kw_args):\n",
      "        \"\"\"\"\"\"gplvm_dimensions: dimensions to learn using gplvm, default -1; i.e. all\"\"\"\"\"\"\n",
      "        GP.__init__(self,**kw_args)\n",
      "        self.gplvm_dimensions = None\n",
      "\n",
      "\n",
      "\n",
      "    def setData(self,gplvm_dimensions=None,**kw_args):\n",
      "        GP.setData(self,**kw_args)\n",
      "        #handle non-informative gplvm_dimensions vector\n",
      "        if gplvm_dimensions is None:\n",
      "            self.gplvm_dimensions = SP.arange(self.x.shape[1])\n",
      "        else:\n",
      "            self.gplvm_dimensions = gplvm_dimensions\n",
      "        \n",
      "    def _update_inputs(self,hyperparams):\n",
      "        \"\"\"\"\"\"update the inputs from gplvm models if supplied as hyperparms\"\"\"\"\"\"\n",
      "        #update\n",
      "        self.x[:,self.gplvm_dimensions] = hyperparams['x']\n",
      "        pass\n",
      "   \n",
      "    def lMl(self,hyperparams,priors=None,**kw_args):\n",
      "        \"\"\"\"\"\"\n",
      "        Calculate the log Marginal likelihood\n",
      "        for the given logtheta.\n",
      "\n",
      "        **Parameters:**\n",
      "\n",
      "        hyperparams : {'covar':CF_hyperparameters, ... }\n",
      "            The hyperparameters for the log marginal likelihood.\n",
      "\n",
      "        priors : [:py:class:`lnpriors`]\n",
      "            the prior beliefs for the hyperparameter values\n",
      "\n",
      "        Ifilter : [bool]\n",
      "            Denotes which hyperparameters shall be optimized.\n",
      "            Thus ::\n",
      "\n",
      "                Ifilter = [0,1,0]\n",
      "\n",
      "            has the meaning that only the second\n",
      "            hyperparameter shall be optimized.\n",
      "\n",
      "        kw_args :\n",
      "            All other arguments, explicitly annotated\n",
      "            when necessary.\n",
      "            \n",
      "        \"\"\"\"\"\"\n",
      "        if 'x' in hyperparams:\n",
      "            self._update_inputs(hyperparams)\n",
      "\n",
      "        #covariance hyper\n",
      "        lMl = self._lMl_covar(hyperparams)\n",
      "\n",
      "        \n",
      "        #account for prior\n",
      "        if priors is not None:\n",
      "            plml = self._lml_prior(hyperparams,priors=priors,**kw_args)\n",
      "            lMl -= SP.array([p[:,0].sum() for p in plml.values()]).sum()\n",
      "        return lMl\n",
      "        \n",
      "\n",
      "    def dlMl(self,hyperparams,priors=None,**kw_args):\n",
      "        \"\"\"\"\"\"\n",
      "        Returns the log Marginal likelihood for the given logtheta.\n",
      "\n",
      "        **Parameters:**\n",
      "\n",
      "        hyperparams : {'covar':CF_hyperparameters, ...}\n",
      "            The hyperparameters which shall be optimized and derived\n",
      "\n",
      "        priors : [:py:class:`lnpriors`]\n",
      "            The hyperparameters which shall be optimized and derived\n",
      "\n",
      "        \"\"\"\"\"\"\n",
      "        # Ideriv : \n",
      "        #      indicator which derivativse to calculate (default: all)\n",
      "\n",
      "        if 'x' in hyperparams:\n",
      "            self._update_inputs(hyperparams)\n",
      "            \n",
      "        RV = self._dlMl_covar(hyperparams)\n",
      "        #\n",
      "        if 'x' in hyperparams:\n",
      "            RV_ = self.dlMl_x(hyperparams)\n",
      "            #update RV\n",
      "            RV.update(RV_)\n",
      "\n",
      "        #prior\n",
      "        if priors is not None:\n",
      "            plml = self._lml_prior(hyperparams,priors=priors,**kw_args)\n",
      "            for key in RV.keys():\n",
      "                RV[key]-=plml[key][:,1]                       \n",
      "        return RV\n",
      "\n",
      "\n",
      "    ####PRIVATE####\n",
      "\n",
      "    def dlMl_x(self,hyperparams):\n",
      "        \"\"\"\"\"\"GPLVM derivative w.r.t. to latent variables\n",
      "        \"\"\"\"\"\"\n",
      "        #grad check of the covariance ? \n",
      "        #grad_checkK(self.covar,hyperparams['covar'],self.x,self.gplvm_dimensions)\n",
      "        \n",
      "        dlMl = SP.zeros([self.n,len(self.gplvm_dimensions)])\n",
      "\n",
      "        #K^-1 * y from cache:\n",
      "        W = self._covar_cache['W']\n",
      "\n",
      "        #the standard procedure would be\n",
      "        #dlMl[n,i] = 0.5*SP.dot(W,dKx_n,i).trace()\n",
      "        #we can calcualte all the derivatives efficiently; see also interface of Kd_dx of covar\n",
      "                   \n",
      "        for i in xrange(len(self.gplvm_dimensions)):\n",
      "            d = self.gplvm_dimensions[i]\n",
      "            #dKx is general, not knowing that we are computing the diagonal:\n",
      "            dKx = self.covar.Kd_dx(hyperparams['covar'],self.x,self.x,d)\n",
      "            dKx_diag = self.covar.Kd_dx_diag(hyperparams['covar'],self.x,d)\n",
      "            #set diagonal\n",
      "            dKx.flat[::(dKx.shape[1]+1)]=dKx_diag\n",
      "            #precalc elementwise product of W and K\n",
      "            WK = W*dKx\n",
      "            if 0:\n",
      "                #explicit calculation, slow!\n",
      "                #this is only in here to see what is done\n",
      "                for n in xrange(self.n):\n",
      "                    dKxn = SP.zeros([self.n,self.n])\n",
      "                    dKxn[n,:] = dKx[n,:]\n",
      "                    dKxn[:,n] = dKx[n,:]\n",
      "                    dlMl[n,i] = 0.5*SP.dot(W,dKxn).trace()\n",
      "                    pass\n",
      "            if 1:\n",
      "                #fast calculation\n",
      "                #we need twice the sum WK because of the matrix structure above, WK.diagonal() accounts for the double counting\n",
      "                dlMl[:,i] = 0.5*( 2*WK.sum(axis=1) - WK.diagonal() )\n",
      "            pass\n",
      "        RV = {'x':dlMl}\n",
      "        return RV\n",
      "        \n",
      "\n",
      "if __name__ =='__main__':\n",
      "    import sys\n",
      "    from pygp.covar import linear, noise, combinators\n",
      "    import pygp.gpr as GPR\n",
      "    import logging as LG\n",
      "    LG.basicConfig(level=LG.DEBUG)\n",
      "    \n",
      "    #1. simulate data\n",
      "    N = 100\n",
      "    K = 3\n",
      "    D = 10\n",
      "\n",
      "    \n",
      "    S = SP.random.randn(N,K)\n",
      "    W = SP.random.randn(D,K)\n",
      "    \n",
      "    Y = SP.dot(W,S.T).T\n",
      "    Y+= 0.5*SP.random.randn(N,D)\n",
      "  \n",
      "    [Spca,Wpca] = PCA(Y,K)\n",
      "    \n",
      "    #reconstruction\n",
      "    Y_ = SP.dot(Spca,Wpca.T)\n",
      "    \n",
      "    #construct GPLVM model\n",
      "    linear_cf = linear.LinearCFISO(n_dimensions=K)\n",
      "    noise_cf = noise.NoiseISOCF()\n",
      "    covariance = combinators.SumCF((linear_cf,noise_cf))\n",
      "\n",
      "\n",
      "    #no inputs here (later SNPs)\n",
      "    X = Spca.copy()\n",
      "    #X = SP.random.randn(N,K)\n",
      "    gplvm = GPLVM(covar_func=covariance,x=X,y=Y)\n",
      "\n",
      "    gpr = GPR.GP(covar_func=covariance,x=X,y=Y[:,0])\n",
      "    \n",
      "    #construct hyperparams\n",
      "    covar = SP.log([1.0,0.1])\n",
      "\n",
      "    #X are hyperparameters, i.e. we optimize over them also\n",
      "\n",
      "    #1. this is jointly with the latent X\n",
      "    X_ = X.copy()\n",
      "    hyperparams = {'covar': covar, 'x': X_}\n",
      "    \n",
      "\n",
      "    #for testing just covar params alone:\n",
      "    #hyperparams = {'covar': covar}\n",
      "    \n",
      "    #evaluate log marginal likelihood\n",
      "    lml = gplvm.lMl(hyperparams=hyperparams)\n",
      "    [opt_model_params,opt_lml]=GPR.optHyper(gplvm,hyperparams,gradcheck=False)\n",
      "    Xo = opt_model_params['x']\n",
      "    \n",
      "\n",
      "    for k in xrange(K):\n",
      "        print SP.corrcoef(Spca[:,k],S[:,k])\n",
      "\n",
      "    for k in xrange(K):\n",
      "        print SP.corrcoef(Xo[:,k],S[:,k])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(splitted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\n",
      "sys\n",
      "finding: .path.append( finding key: sys.path.append\n",
      "\n",
      "\n",
      "from pygp.gpr import GP,_solve_chol,optHyper\n",
      "GP\n",
      "finding: .__init__( finding key: pygp.gpr.GP.__init__\n",
      "finding: .setData( finding key: pygp.gpr.GP.setData\n",
      "finding: ( finding key: pygp.gpr.GP\n",
      "_solve_chol\n",
      "optHyper\n",
      "finding: ( finding key: pygp.gpr.optHyper\n",
      "\n",
      "\n",
      "import scipy as SP\n",
      "scipy as SP\n",
      "finding: .zeros( finding key: scipy.zeros\n",
      "finding: .zeros( finding key: scipy.zeros\n",
      "finding: .dot(SP.diag( finding key: scipy.dotSP.diag\n",
      "finding: .arange( finding key: scipy.arange\n",
      "finding: .array([p[:,0].sum() for p in plml.values()]).sum( finding key: scipy.array[p[:,0].sum) for p in plml.values)]).sum\n",
      "finding: .zeros([self.n,len( finding key: scipy.zeros[self.n,len\n",
      "finding: .dot(W,dKx_n,i).trace( finding key: scipy.dotW,dKx_n,i).trace\n",
      "finding: .zeros( finding key: scipy.zeros\n",
      "finding: .dot(W,dKxn).trace( finding key: scipy.dotW,dKxn).trace\n",
      "finding: .random.randn( finding key: scipy.random.randn\n",
      "finding: .random.randn( finding key: scipy.random.randn\n",
      "finding: .dot( finding key: scipy.dot\n",
      "finding: .random.randn( finding key: scipy.random.randn\n",
      "finding: .dot( finding key: scipy.dot\n",
      "finding: .random.randn( finding key: scipy.random.randn\n",
      "finding: .log( finding key: scipy.log\n",
      "finding: .corrcoef( finding key: scipy.corrcoef\n",
      "finding: .corrcoef( finding key: scipy.corrcoef\n",
      "\n",
      "\n",
      "import numpy.linalg as linalg\n",
      "numpy.linalg as linalg\n",
      "finding: .svd( finding key: numpy.linalg.svd\n",
      "\n",
      "\n",
      "import pdb\n",
      "pdb\n",
      "finding: .set_trace( finding key: pdb.set_trace\n",
      "\n",
      "\n",
      "import copy\n",
      "copy\n",
      "finding: ( finding key: copy\n",
      "finding: ( finding key: copy\n",
      "finding: ( finding key: copy\n",
      "\n",
      "\n",
      "from pyplot import *\n",
      "*\n",
      "\n",
      "\n",
      "import sys\n",
      "sys\n",
      "finding: .path.append( finding key: sys.path.append\n",
      "\n",
      "\n",
      "from pygp.covar import linear, noise, combinators\n",
      "linear\n",
      "finding: .LinearCFISO( finding key: pygp.covar.linear.LinearCFISO\n",
      "noise\n",
      "finding: .NoiseISOCF( finding key: pygp.covar.noise.NoiseISOCF\n",
      "combinators\n",
      "finding: .SumCF(( finding key: pygp.covar.combinators.SumCF\n",
      "\n",
      "\n",
      "import pygp.gpr as GPR\n",
      "pygp.gpr as GPR\n",
      "finding: .GP( finding key: pygp.gpr.GP\n",
      "finding: .optHyper( finding key: pygp.gpr.optHyper\n",
      "\n",
      "\n",
      "import logging as LG\n",
      "logging as LG\n",
      "finding: .basicConfig( finding key: logging.basicConfig\n",
      "\n",
      "\n",
      "from __future__ import print_function\n",
      "print_function\n",
      "\n",
      "\n",
      "import pdb\n",
      "pdb\n",
      "\n",
      "\n",
      "import torch\n",
      "torch\n",
      "\n",
      "\n",
      "import torch.nn as nn\n",
      "torch.nn as nn\n",
      "\n",
      "\n",
      "import torch.nn.functional as F\n",
      "torch.nn.functional as F\n",
      "\n",
      "\n",
      "from torch.autograd import Variable\n",
      "Variable\n",
      "\n",
      "\n",
      "import argparse\n",
      "argparse\n",
      "\n",
      "\n",
      "import torchvision\n",
      "torchvision\n",
      "\n",
      "\n",
      "import torchvision.transforms as transforms\n",
      "torchvision.transforms as transforms\n",
      "\n",
      "\n",
      "import torch.optim as optim\n",
      "torch.optim as optim\n",
      "\n",
      "\n",
      "from torch.utils.data import Dataset\n",
      "Dataset\n",
      "\n",
      "\n",
      "import sys\n",
      "sys\n",
      "\n",
      "\n",
      "import matplotlib\n",
      "matplotlib\n",
      "finding: .use( finding key: matplotlib.use\n",
      "\n",
      "\n",
      "import h5py\n",
      "h5py\n",
      "\n",
      "\n",
      "import numpy\n",
      "numpy\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "numpy as np\n",
      "\n",
      "\n",
      "import os\n",
      "os\n",
      "\n",
      "\n",
      "import random\n",
      "random\n",
      "\n",
      "\n",
      "import cPickle as pkl\n",
      "cPickle as pkl\n",
      "\n",
      "\n",
      "import datetime\n",
      "datetime\n",
      "\n",
      "\n",
      "import time\n",
      "time\n",
      "\n",
      "\n",
      "import gzip\n",
      "gzip\n",
      "\n",
      "\n",
      "from six.moves import urllib\n",
      "urllib\n",
      "\n",
      "\n",
      "from six.moves import xrange  # pylint: disabl\n",
      "xrange  # pylint: disabl\n",
      "\n",
      "\n",
      "from sklearn import svm, datasets\n",
      "svm\n",
      "datasets\n",
      "\n",
      "\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "precision_recall_curve\n",
      "\n",
      "\n",
      "from sklearn.metrics import average_precision_score\n",
      "average_precision_score\n",
      "\n",
      "\n",
      "import glob\n",
      "glob\n",
      "\n",
      "\n",
      "import fnmatch\n",
      "fnmatch\n",
      "\n",
      "\n",
      "import md5\n",
      "md5\n",
      "\n",
      "\n",
      "import hashlib\n",
      "hashlib\n",
      "\n",
      "\n",
      "import aifc\n",
      "aifc\n",
      "\n",
      "\n",
      "import scipy\n",
      "scipy\n",
      "\n",
      "\n",
      "from scipy.fftpack import fft\n",
      "fft\n",
      "\n",
      "\n",
      "import csv\n",
      "csv\n",
      "\n",
      "\n",
      "import cPickle as pickle\n",
      "cPickle as pickle\n",
      "\n",
      "\n",
      "import matplotlib.patches as patches\n",
      "matplotlib.patches as patches\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "matplotlib.pyplot as plt\n",
      "finding: .ion( finding key: matplotlib.pyplot.ion\n",
      "\n",
      "\n",
      "from collections import Counter\n",
      "Counter\n",
      "\n",
      "\n",
      "from sklearn.metrics import confusion_matrix\n",
      "confusion_matrix\n",
      "\n",
      "\n",
      "from sklearn.metrics import roc_curve\n",
      "roc_curve\n",
      "\n",
      "\n",
      "from sklearn import metrics\n",
      "metrics\n",
      "\n",
      "\n",
      "from sklearn import svm, datasets\n",
      "svm\n",
      "datasets\n",
      "\n",
      "\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "precision_recall_curve\n",
      "\n",
      "\n",
      "from sklearn.metrics import average_precision_score\n",
      "average_precision_score\n",
      "\n",
      "\n",
      "import dcerpc as __dcerpc\n",
      "dcerpc as __dcerpc\n",
      "\n",
      "\n",
      "import talloc as __talloc\n",
      "talloc as __talloc\n",
      "\n",
      "\n",
      "import codecs\n",
      "codecs\n",
      "finding: .open(os.path.join( finding key: codecs.openos.path.join\n",
      "\n",
      "\n",
      "import os\n",
      "os\n",
      "finding: .path.abspath( finding key: os.path.abspath\n",
      "finding: .path.abspath(os.path.dirname( finding key: os.path.abspathos.path.dirname\n",
      "finding: .path.join( finding key: os.path.join\n",
      "\n",
      "\n",
      "import re\n",
      "re\n",
      "finding: .search(r\"\"^__version__ = ['\\\"\"]( finding key: re.searchr\"\"^__version__ = ['\\\"\"]\n",
      "\n",
      "\n",
      "import sys\n",
      "sys\n",
      "finding: .path.insert( finding key: sys.path.insert\n",
      "\n",
      "\n",
      "from mechanize import ParseResponse, urlopen, URLError\n",
      "ParseResponse\n",
      "finding: ( finding key: mechanize.ParseResponse\n",
      "urlopen\n",
      "finding: ( finding key: mechanize.urlopen\n",
      "finding: ( finding key: mechanize.urlopen\n",
      "finding: ( finding key: mechanize.urlopen\n",
      "URLError\n",
      "\n",
      "\n",
      "from urllib2 import URLError as PlainURLError\n",
      "URLError as PlainURLError\n",
      "\n",
      "\n",
      "from urllib2 import urlopen as Plainurlopen\n",
      "urlopen as Plainurlopen\n",
      "finding: ( finding key: urllib2.urlopen\n",
      "\n",
      "\n",
      "{'sys.path.append': 2, 'pygp.gpr.GP.__init__': 1, 'pygp.gpr.GP.setData': 1, 'pygp.gpr.GP': 2, 'pygp.gpr.optHyper': 2, 'scipy.zeros': 3, 'scipy.dotSP.diag': 1, 'scipy.arange': 1, 'scipy.array[p[:,0].sum) for p in plml.values)]).sum': 1, 'scipy.zeros[self.n,len': 1, 'scipy.dotW,dKx_n,i).trace': 1, 'scipy.dotW,dKxn).trace': 1, 'scipy.random.randn': 4, 'scipy.dot': 2, 'scipy.log': 1, 'scipy.corrcoef': 2, 'numpy.linalg.svd': 1, 'pdb.set_trace': 1, 'copy': 3, 'pygp.covar.linear.LinearCFISO': 1, 'pygp.covar.noise.NoiseISOCF': 1, 'pygp.covar.combinators.SumCF': 1, 'logging.basicConfig': 1, 'matplotlib.use': 1, 'matplotlib.pyplot.ion': 1, 'codecs.openos.path.join': 1, 'os.path.abspath': 1, 'os.path.abspathos.path.dirname': 1, 'os.path.join': 1, 're.searchr\"\"^__version__ = [\\'\\\\\"\"]': 1, 'sys.path.insert': 1, 'mechanize.ParseResponse': 1, 'mechanize.urlopen': 3, 'urllib2.urlopen': 1}\n"
     ]
    }
   ],
   "source": [
    "usage = {}\n",
    "for snippet in splitted[:5]:\n",
    "    for imp in re.findall(r'\\n *import .+|\\n *from .+ import .+', snippet):\n",
    "        import_line = imp.strip()\n",
    "        snippet = snippet.replace(import_line, '')\n",
    "        import_v_from = re.split(r' +', import_line)[0]\n",
    "        lib_name = re.split(r' +', import_line)[1]\n",
    "        lib_alias, mo_alias, methods_objects = '', '', ''\n",
    "\n",
    "        print(import_line)\n",
    "        if import_v_from == 'import':\n",
    "            libs = re.split(r' ?, ?', import_line.replace('import', '').strip())\n",
    "            for l in libs:\n",
    "                print(l)\n",
    "                if ' as ' in l:\n",
    "                    lib_name = re.split(r' +as +', l)[0].strip()\n",
    "                    lib_alias = re.split(r' +as +', l)[1].strip()\n",
    "                    for finding in re.findall(lib_alias+r'(\\(|\\..+\\()', snippet):\n",
    "                        finding_key = lib_name + finding.replace('(', '')\n",
    "                        print('finding:', finding, 'finding key:', finding_key)\n",
    "                        if finding_key not in usage:\n",
    "                            usage[finding_key] = 1\n",
    "                        else:\n",
    "                            usage[finding_key] += 1\n",
    "                else:\n",
    "                    lib_name = l.strip()\n",
    "                    for finding in re.findall(lib_name+r'(\\(|\\..+\\()', snippet):\n",
    "                        finding_key = lib_name + finding.replace('(', '')\n",
    "                        print('finding:', finding, 'finding key:', finding_key)\n",
    "                        if finding_key not in usage:\n",
    "                            usage[finding_key] = 1\n",
    "                        else:\n",
    "                            usage[finding_key] += 1\n",
    "        elif import_v_from == 'from':\n",
    "            methods_objects = re.split(r' ?, ?', import_line.split('import')[-1].strip())\n",
    "            for mo in methods_objects:\n",
    "                print(mo)\n",
    "                if '*' == mo:\n",
    "                    continue\n",
    "                if ' as ' in mo:\n",
    "                    mo_name = re.split(r' +as +', mo)[0].strip()\n",
    "                    mo_alias = re.split(r' +as +', mo)[1].strip()\n",
    "                    for finding in re.findall(mo_alias+r'(\\(|\\..+\\()', snippet):\n",
    "                        finding_key = lib_name + '.' + mo_name + finding.replace('(', '')\n",
    "                        print('finding:', finding, 'finding key:', finding_key)\n",
    "                        if finding_key not in usage:\n",
    "                            usage[finding_key] = 1\n",
    "                        else:\n",
    "                            usage[finding_key] += 1\n",
    "                else:\n",
    "                    mo_name = mo.strip()\n",
    "                    for finding in re.findall(mo_name+r'(\\(|\\..+\\()', snippet):\n",
    "                        finding_key = lib_name + '.' + mo_name + finding.replace('(', '')\n",
    "                        print('finding:', finding, 'finding key:', finding_key)\n",
    "                        if finding_key not in usage:\n",
    "                            usage[finding_key] = 1\n",
    "                        else:\n",
    "                            usage[finding_key] += 1\n",
    "        print('\\n')\n",
    "\n",
    "print(usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.sdfa.dfgh() sdf()   sdf.asd(', '.asdf.(']\n"
     ]
    }
   ],
   "source": [
    "foo = '''asdfsadf sdf.sdfa.dfgh() sdf()   sdf.asd()\n",
    "sdf.asdf.()'''\n",
    "bar = 'sdf'\n",
    "print(re.findall(bar+r'(\\(|\\..+\\()', foo))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96a5d84734c2fcc0f1c4e29111ed2080412c48bc33f767562c76143427b08200"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
