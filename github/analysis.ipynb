{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('G:\\python_github_dump\\github_python_dump000000000001', encoding='utf8') as f:\n",
    "    dump = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = dump.replace('\",false,1', '')\n",
    "splitted = re.split(r'[a-z0-9]{40},\\d+,\"', dump)\n",
    "splitted = splitted[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "import json\n",
      "import os, sys\n",
      "import itertools\n",
      "import time\n",
      "import codecs\n",
      "from clean_data import Cleaner\n",
      "\n",
      "# the valid uptodate event_bulletinurl.csv in OUT_DATA folder\n",
      "LINK_TO_EVENT=\"\"event_bulletinurl.csv\"\"\n",
      "CLEANING=\"\"cleaning_sections_metadata.json\"\"\n",
      "\n",
      "#ENB_DATA = \"\"enb_section_jsons\"\"\n",
      "ENB_DATA = \"\"enb_section_jsons-topiked\"\"\n",
      "OUT_DATA = \"\"metadata_overview\"\"\n",
      "if len(sys.argv) > 1:\n",
      "    ENB_DATA = sys.argv[1]\n",
      "if len(sys.argv) > 2:\n",
      "    OUT_DATA = sys.argv[2]\n",
      "if len(sys.argv) > 3:\n",
      "    LINK_TO_EVENT = sys.argv[3]\n",
      "if len(sys.argv) > 4:\n",
      "    CLEANING = sys.argv[4]\n",
      "\n",
      "format_txt = lambda x: ('\"\"' + x.replace('\"\"', '\"\"\"\"').replace(\"\"\\n\"\", \"\" \"\").replace(\"\"\\r\"\", \"\"\"\") + '\"\"').encode('utf-8')\n",
      "format_spe = lambda x: format_txt(\"\"|\"\".join(x))\n",
      "format_field = lambda x: format_txt(x) if type(x) == unicode else format_spe(x)\n",
      "\n",
      "data_index={}\n",
      "\n",
      "# prepare index for LINK SECTIONS TO EVENT task\n",
      "add_eventid=False\n",
      "if os.path.exists(os.path.join(OUT_DATA,LINK_TO_EVENT)):\n",
      "    with codecs.open(os.path.join(OUT_DATA,LINK_TO_EVENT),'r',\"\"utf8\"\") as f:\n",
      "        url_eventid=dict(reversed(id_url.replace(\"\"\\r\\n\"\",\"\"\"\").split(\"\",\"\")) for id_url in f.readlines()[1:])\n",
      "        add_eventid=True\n",
      "        url_eventid[\"\"http://www.iisd.ca/vol12/enb12300e.html\"\"]=url_eventid[\"\"http://www.iisd.ca/vol12/enb12300.html\"\"]\n",
      "        url_eventid[\"\"http://www.iisd.ca/vol12/enb12603e.html\"\"]=url_eventid[\"\"http://www.iisd.ca/vol12/enb125603e.html\"\"]\n",
      "\n",
      "# prepare index for CLEANING SECTIONS\n",
      "try:\n",
      "    cleaner = Cleaner(os.path.join(OUT_DATA, CLEANING))\n",
      "    clean_sections=True\n",
      "except:\n",
      "    clean_sections=False\n",
      "\n",
      "headers = [\n",
      "    \"\"id\"\",\n",
      "    \"\"title\"\",\n",
      "    \"\"actors\"\",\n",
      "    \"\"countries\"\",\n",
      "    \"\"topics\"\",\n",
      "    \"\"url\"\",\n",
      "    \"\"event_id\"\",\n",
      "    \"\"track\"\",\n",
      "    \"\"format\"\",\n",
      "    \"\"date\"\",\n",
      "    \"\"year\"\",\n",
      "    \"\"abstract\"\"\n",
      "]\n",
      "\n",
      "def shorten(t):\n",
      "    sp_pos = 0\n",
      "    spaces = 0\n",
      "    while sp_pos != -1:\n",
      "        sp_pos = t.find(\"\" \"\", sp_pos + 1)\n",
      "        spaces += 1\n",
      "        if spaces > 20:\n",
      "            return t[:sp_pos] + u\"\"â€¦\"\"\n",
      "    return t\n",
      "\n",
      "section_metadata = []\n",
      "section_fulldata = []\n",
      "for directory,subdir,filenames in os.walk(ENB_DATA):\n",
      "    for f in filenames:\n",
      "        if f.split(\"\".\"\")[-1]==\"\"json\"\":\n",
      "            with open(os.path.join(directory,f),\"\"r\"\") as jsonfile:\n",
      "                #load a section\n",
      "                data=json.load(jsonfile)\n",
      "\n",
      "                # clean\n",
      "                if clean_sections:\n",
      "                    data=cleaner.clean_section(data)\n",
      "\n",
      "                # process date\n",
      "                import locale\n",
      "                locale.setlocale(locale.LC_TIME, \"\"en_US.utf8\"\")\n",
      "                date=time.strptime(data[\"\"enb_end_date\"\"],\"\"%d-%b-%y\"\")\n",
      "                epoch_millisecond=time.mktime(date)*1000\n",
      "                year=time.strftime(\"\"%Y\"\",date)\n",
      "\n",
      "                csv_data=[\n",
      "                    data[\"\"id\"\"],\n",
      "                    data[\"\"section_title\"\"],\n",
      "                    [act.replace('&', 'and') for act in data[\"\"actors\"\"]],\n",
      "                    [cou.replace('&', 'and') for cou in data[\"\"countries\"\"]],\n",
      "                    [top.replace('&', 'and') for top in data[\"\"topics\"\"]],\n",
      "                    data[\"\"enb_url\"\"],\n",
      "                    u\"\"\"\" if not add_eventid else url_eventid[data[\"\"enb_url\"\"]],\n",
      "                    data[\"\"type\"\"].replace('&', 'and'),\n",
      "                    data[\"\"subtype\"\"].replace('&', 'and'),\n",
      "                    unicode(epoch_millisecond),\n",
      "                    unicode(year),\n",
      "                    shorten(data[\"\"sentences\"\"][0].replace('\\u0092', \"\"'\"\"))\n",
      "                    ]\n",
      "\n",
      "                csv_fulldata = list(csv_data)\n",
      "                csv_fulldata[-1] = \"\" \"\".join(data[\"\"sentences\"\"]).replace('\\u0092', \"\"'\"\")\n",
      "                section_fulldata.append(csv_fulldata)\n",
      "\n",
      "                # filtering\n",
      "                if not clean_sections or cleaner.keep_section(data):\n",
      "                    section_metadata.append(csv_data)\n",
      "\n",
      "                    # create index and filter sentences out\n",
      "                    for k,v in data.iteritems():\n",
      "                        if k!=\"\"sentences\"\":\n",
      "                            data_index[k]=[v] if k not in data_index else data_index[k]+[v]\n",
      "\n",
      "\n",
      "with open(os.path.join(OUT_DATA,\"\"sections_metadata.csv\"\"), \"\"w\"\") as metadata_file:\n",
      "    print >> metadata_file, (\"\",\"\".join(headers)).encode('utf-8')\n",
      "    section_metadata.sort(key=lambda x: (x[6], x[0]))\n",
      "    for l in section_metadata:\n",
      "        for field in l:\n",
      "            try:\n",
      "                format_field(field)\n",
      "            except:\n",
      "                print \"\"failed to format this value :%s\"\"%field\n",
      "                exit()\n",
      "        print >> metadata_file, \"\",\"\".join(format_field(field) for field in l)\n",
      "\n",
      "headers[-1] = \"\"fulltext\"\"\n",
      "with open(os.path.join(OUT_DATA,\"\"sections_fulldata.csv\"\"), \"\"w\"\") as fulldata_file:\n",
      "    print >> fulldata_file, (\"\",\"\".join(headers)).encode('utf-8')\n",
      "    section_fulldata.sort(key=lambda x: (x[6], x[0]))\n",
      "    for l in section_fulldata:\n",
      "        for field in l:\n",
      "            try:\n",
      "                format_field(field)\n",
      "            except:\n",
      "                print \"\"failed to format this value :%s\"\"%field\n",
      "                exit()\n",
      "        print >> fulldata_file, \"\",\"\".join(format_field(field) for field in l)\n",
      "\n",
      "\n",
      "key_usage={}\n",
      "val_usage={}\n",
      "\n",
      "for k in data_index:\n",
      "    key_usage[k]=len(data_index[k])\n",
      "    if k in [\"\"actors\"\",\"\"countries\"\",\"\"topics\"\"]:\n",
      "        values=[_ for v in data_index[k] for _ in v]\n",
      "    else:\n",
      "        values=data_index[k]\n",
      "    val_usage[k]=values if k not in val_usage else val_usage[k]+values\n",
      "\n",
      "val_stat={}\n",
      "for k in val_usage:\n",
      "    val_stat[k]={}\n",
      "    for value,values in itertools.groupby(sorted(val_usage[k])):\n",
      "        val_stat[k][value if value != [] else \"\"\"\"]=len([_ for _ in values])\n",
      "\n",
      "    val_stat[k]=sorted(val_stat[k].iteritems(),key=lambda t:t[1],reverse=True)\n",
      "\n",
      "\n",
      "with open(os.path.join(OUT_DATA,\"\"key_stat.json\"\"),\"\"w\"\") as f:\n",
      "    json.dump(key_usage,f,indent=4)\n",
      "with open(os.path.join(OUT_DATA,\"\"val_stat.json\"\"),\"\"w\"\") as f:\n",
      "    json.dump(val_stat,f,indent=4)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(splitted[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_parentheses(input_string):\n",
    "    parens = []\n",
    "    opening_p = []\n",
    "    for index, ch in enumerate(input_string):\n",
    "        if ch == '(':\n",
    "            opening_p.append(index)\n",
    "        elif ch == ')':\n",
    "            op = opening_p.pop()\n",
    "            if len(opening_p) == 0:\n",
    "                parens.append(input_string[op:index+1])\n",
    "    for p in parens:\n",
    "        input_string = input_string.replace(p, '')\n",
    "    return input_string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_imports(imports):\n",
    "    '''Returns list of tuples containing import name, import search string pairs'''\n",
    "    imports_parsed = set()\n",
    "    for import_line in imports:\n",
    "        import_v_from = re.split(r' +', import_line)[0]\n",
    "        if import_v_from == 'import':\n",
    "            libs = re.split(r' ?, ?', import_line.replace('import', '').strip())\n",
    "            for l in libs:\n",
    "                if ' as ' in l:\n",
    "                    lib_name = re.split(r' +as +', l)[0].strip()\n",
    "                    lib_alias = re.split(r' +as +', l)[1].strip()\n",
    "                    imports_parsed.add((lib_name, lib_alias))\n",
    "                else:\n",
    "                    imports_parsed.add((l.strip(), l.strip()))\n",
    "        elif import_v_from == 'from':\n",
    "            lib_name = import_line.split('import')[0].replace('from ', '').strip()\n",
    "            methods_objects = re.split(r' ?, ?', import_line.split('import')[-1].strip())\n",
    "            for mo in methods_objects:\n",
    "                if '*' == mo:\n",
    "                    continue\n",
    "                if ' as ' in mo:\n",
    "                    mo_name = re.split(r' +as +', mo)[0].strip()\n",
    "                    mo_alias = re.split(r' +as +', mo)[1].strip()\n",
    "                    imports_parsed.add((f'{lib_name}.{mo_name}', mo_alias))\n",
    "                else:\n",
    "                    imports_parsed.add((f'{lib_name}.{mo.strip()}', mo.strip()))\n",
    "    return imports_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_parentheses(input_string):\n",
    "    return input_string.count('(') == input_string.count(')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments_docstrings(input_string):\n",
    "    input_string = re.sub(r'#.*\\n', '\\n', input_string)\n",
    "    start = None\n",
    "    for match in re.finditer(r'[\"\"\"+|\\'\\'\\'+]+', input_string):\n",
    "        if not start:\n",
    "            start = match.span()[0]\n",
    "        else:\n",
    "            to_replace = input_string[start: match.span()[1]]\n",
    "            input_string = input_string.replace(to_replace, len(to_replace)*' ')\n",
    "            start = None\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_objects(input_string, imports_list):\n",
    "    objects = set()\n",
    "    for imp in imports_list:\n",
    "        for f in re.findall(r'.+= *{}'.format(imp[1]), input_string):\n",
    "            objects.add((f'{imp[0]}_derivative', f.split('=')[0].strip()))\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_usage(input_string, names):\n",
    "    for line in input_string.replace('\\\\\\n', ' ').splitlines():\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('clean_data_Cleaner_object', 'cleaner'), ('json_object', 'data'), ('sys_object', 'LINK_TO_EVENT'), ('time_object', 'date'), ('sys_object', 'ENB_DATA'), ('time_object', 'year'), ('sys_object', 'CLEANING'), ('sys_object', 'OUT_DATA'), ('time_object', 'epoch_millisecond')}\n"
     ]
    }
   ],
   "source": [
    "imports = []\n",
    "for imp in re.findall(r'\\n *import .+|\\n *from .+ import .+', splitted[15]):\n",
    "    import_line = imp.split('#')[0].strip()\n",
    "    snippet = snippet.replace(import_line, '')\n",
    "    imports.append(import_line)\n",
    "imports = parse_imports(imports)\n",
    "print(find_objects(splitted[15], imports))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96a5d84734c2fcc0f1c4e29111ed2080412c48bc33f767562c76143427b08200"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
